"""This is the library for our UCD Decision Analytics MCDA project.

These libraries read the relevant data from Google Spreadsheet. Format it into
a data dictionary and then converts the dictionaries ito relevant tree level
matrices.
"""
import json
import gspread
from oauth2client.client import SignedJwtAssertionCredentials
import numpy
import pandas
from collections import namedtuple
import networkx
import matplotlib.pyplot as plt

Edge = namedtuple('Edge', ['source', 'target', 'relative_weight'])


def GetDataFromGoogleSpreadsheet(spreadsheet_key):
  """Reads data from the relevant google speadsheet.

  Reads the data into a list of dictionaries. Required the .json pass file.

  Returns:
    A list of dictionaries.
  """
  json_key = json.load(open('MCDA-UCD-8b0b964e491d.json'))
  scope = ['https://spreadsheets.google.com/feeds']
  credentials = SignedJwtAssertionCredentials(
      json_key['client_email'], json_key['private_key'].encode(), scope)
  gc = gspread.authorize(credentials)
  wks = gc.open_by_key(spreadsheet_key).get_worksheet(0)
  cell_list = wks.get_all_records()
  return cell_list


def GetStructuredDataRollup(data, scoring_default=True):
  """Restructures the spreadsheet data into a nested structure including scores.

  Args:
    data: a Google spreadsheet cell list.
    scoring_default: A boolean of whether to use the default scoring system or
      the alternative.

  Returns:
    A list of dictionairies with each dict corresponding to a given spreadsheet
      row. 
  """
  return_list = []
  split_keys = set()
  for data_dict in data:
    map_values = {}
    for key, value in data_dict.iteritems():
      if key != 'Timestamp':
        keyword1, keyword2 = key.split(' or ')
        # This is where I assign the scores that ends up in the final matrices.
        map_values[key] = ScoringDictionary(keyword1, keyword2, value,
                                            scoring_default)
    return_list.append(map_values)
  return return_list


# TODO(Max): Make GetStructuredDataRollup take a scoring function as an argument
#            and spit ScoringDictionary.
def ScoringDictionary(keyword1, keyword2, value, default=True):
  """Populates scoring dictionary for the given keyword pair and score.

  Args:
    keyword1: A string keyword.
    keyword2: A string keyword.
    value: A numeric score
    default: Boolean, whether to use the default scoring method or the 
        alternative.

  Returns:
    A dict mapping the keywords to their relevant numeric scores.
  """
  if default:
    return  {keyword1: 8-value, keyword2: value}
  else:
    return  {keyword1: max(4-value, 0), keyword2: max(value-4, 0)}


def ConstructQuestionScoringMatrix(question_set, structured_data_dict,
                                   scoring_default=True):
  """Constructs a numerical matrix from the provided question set and data dict.

  Args:
    question_set: A set of string keywords.
    structured_data_dict: A structured data dict generated by
      GetStructuredDataRollup.
    scoring_default: Boolean of whether to use default or alternative scores.

  Returns:
    A pandas DataFrame object with rows and columns corresponding to the
      questions in the input question set.
  """
  n = len(question_set)
  question_list = sorted([question for question in question_set])
  if scoring_default:
    zero_mat = numpy.zeros((n, n))
  else:
    zero_mat = (numpy.ones((n, n))-numpy.identity(n))*1
  df = pandas.DataFrame(zero_mat, index=question_list, columns=question_list)
  for row in structured_data_dict:
    for key, value in row.iteritems():
      # key is the question and value is a dict of keyword to score.
      keyword1, keyword2 = key.split(' or ')
      # Just need to check the first of the or words
      if keyword1 in question_set:
        df[keyword1][keyword2] += value[keyword2]
        df[keyword2][keyword1] += value[keyword1]
  return df


def NormalizeDataFrame(data_frame):
  """Divides all columns in a DataFrame by the sum of that column.

  Converts column of arbitrary scores to one in which all columns sum to 1.
  Necessary preprocessing before Markov chains.

  Args:
    data_frame: a pandas DataFrame object.

  Returns:
    A pandas DataFrame object with normalized scores.
  """
  new_df = data_frame /data_frame.sum()
  return new_df


def GetMarkovScoresList(question_square_matrix):
  """Takes our score matrix and converts in to a list of final scores.

  Args:
    question_square_matrix: A square question matrix with its respective scores
      as populated by ConstructQuestionScoringMatrix above.

  Returns:
    A list of tuples of typ (string label and numerical score).
  """
  n = len(question_square_matrix)
  if n==2:
    labels = list(question_square_matrix.index)
    tempframe = question_square_matrix/question_square_matrix.sum().sum()
    scores = ( tempframe[labels[1]][0], tempframe[labels[0]][1])
    return zip(labels, scores)
  # TODO(Max) Why Normalize here and seperately?
  NormalizedSquareMatrix = NormalizeDataFrame(question_square_matrix)
  transition_matrix = numpy.linalg.matrix_power(NormalizedSquareMatrix, 1000)
  # Need an n * 1 matrix of 1/n to multiply by the transition matrix
  fractional_score_matrix = numpy.ones((n, 1))*(1/(n*1.0))
  final_score = numpy.dot(transition_matrix, fractional_score_matrix)
  labels = list(NormalizedSquareMatrix.index)
  output = zip(labels, final_score)
  return output


def _GenerateGraphEdges(source_node, markov_score_list):
  """Creates a list of Edge named tuples from a given node with weights.

  Reads the score list and returns a list of Edges.

  Args:
    source_node:
    markov_score_list:

  Returns:
    A list of edges.
  """
  edge_list = []
  for score in markov_score_list:
    edge_list.append(Edge(source_node, score[0], score[1]))
  return edge_list


def GenerateCompleteGraph(question_dict, structured_data_dict,
                          scoring_default=True):
  """Generates all the edges for the given network.

  Args:
    question_dict: A dict mapping questions to question sets.
    structured_data_dict: A data dictionary from GetStructuredDataRollup above.
    scoring_default: Boolean whether to use default or alternative scoring.

  Returns:
    A list of all the edges in the network.
  """
  edge_list = []
  for key, value in question_dict.iteritems():
    scores = CalculateScores(value, structured_data_dict, False, scoring_default
                            )
    edge_list.extend(_GenerateGraphEdges(key, scores))
  return edge_list


def CalculateScores(question_set, structured_data_dict, verbose=True,
                    scoring_default=True):
  """Calculates scores for a given question set.

  Args:
    question_set: A set of string questions.
    structured_data_dict: A data dictionary from GetStructuredDataRollup above.
    verbose: Boolean whether to print output.
    scoring_default: Boolean whether to use default or alternative scoring.

  Returns:
    The final score list.
  """
  if verbose:
    print '\n'
    print question_set
  dataframe = ConstructQuestionScoringMatrix(question_set, structured_data_dict,
                                             scoring_default)
  if verbose:
    print dataframe
    print '\n'
  norm_dataframe = NormalizeDataFrame(dataframe)
  if verbose:
    print norm_dataframe
    print '\n'
  final_scores = GetMarkovScoresList(dataframe)
  if verbose:
    print final_scores
    print '\n'
  return final_scores


def GenerateNetworkGraph(question_dict, structured_data_dict, scoring_default):
  """Assemble a networkx DiGraph from the provided data.

  Args:
    question_dict: A dictionary mapping questions to question sets.
    structured_data_dict: A data dictionary from GetStructuredDataRollup above.
    scoring_default: Boolean whether to use default or alternative scoring.

  Returns:
    A networkx edge weighted DiGraph.
  """
  graph = networkx.DiGraph()
  all_edges = GenerateCompleteGraph(
      question_dict, structured_data_dict, scoring_default)
  for edge in all_edges:
    graph.add_edge(edge.source, edge.target, weight=edge.relative_weight)
  return graph


def PlotNetworkGraph(graph):
  """Plots the provided networkx graph.

  Args:
    graph: A networkx graph object.
  """
  pos=networkx.graphviz_layout(graph, prog='dot')
  networkx.draw_networkx(graph, pos=pos, ax=None, with_labels=True, font_size=7)
  labels = networkx.get_edge_attributes(graph,'weight')
  networkx.draw_networkx_edge_labels(graph, pos, edge_labels=labels,
                                     font_size=7)
  plt.draw()
  plt.show()


def GetEndLevelWeights(graph):
  """Crawls entire network from the top to get the end level relative weights.

  Args:
    graph: A networkx Graph object.

  Returns:
    A sorted list of tuples of (Node name, score), sorted by descending score.
  """
  weights_list = []
  _CrawlNetwork(graph,'ROOT', 1.0, weights_list)
  return sorted(weights_list, key=lambda tup: tup[1], reverse=True)


def _CrawlNetwork(graph, node_name, weight, return_tuple_list):
  """Recursively crawl graph to find end nodes and return them and their scores.

  Work from startig node and check its neighbors. If it has any crawl them
  instead. If not append their value and weight to the input list. The new
  weight is their relative weight, calculated by multiplying parent weight by
  that branch weight.

  Args:
    graph: A networkx Graph object.
    node_name: A string name of the node to crawl from.
    weight: A numeric weight.
    return_tuple_list: The input list is modified in the function call.

  Returns:
    Nothing. Modifies input value return_tuple_list by appending tuples of type 
      (node_name, numeric weight).
  """
  successors = graph.neighbors(node_name)
  if successors:
    for child_node in successors:
      _CrawlNetwork(
          graph, child_node, weight*graph[node_name][child_node]['weight'],
          return_tuple_list)
  else:
    return_tuple_list.append((node_name, weight))
